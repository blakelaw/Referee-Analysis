{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#Imports\n",
    "from scipy.stats import ttest_rel\n",
    "import numpy as np\n",
    "import sqlite3 as sql\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LogisticRegression"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:06:32.661637Z",
     "start_time": "2023-09-17T20:06:32.657302Z"
    }
   },
   "id": "78b22c83bc8a40f7"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "#Connect to SQL database\n",
    "conn = sql.connect('nba.sqlite') # create connection object to database\n",
    "df = pd.read_sql('select * from combined_all2', conn)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:06:33.035644Z",
     "start_time": "2023-09-17T20:06:32.659568Z"
    }
   },
   "id": "6a025dbb05910499"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:06:33.278417Z",
     "start_time": "2023-09-17T20:06:33.036579Z"
    }
   },
   "outputs": [],
   "source": [
    "#Convert W to 1 and L to 0\n",
    "df['home_result'] = df['wl_home'].apply(lambda x: 1 if x == 'W' else 0)\n",
    "df['away_result'] = df['wl_home'].apply(lambda x: 0 if x == 'W' else 1)\n",
    "\n",
    "#Create year column\n",
    "df['game_date'] = pd.to_datetime(df['game_date'])\n",
    "df['year'] = df['game_date'].dt.year\n",
    "\n",
    "# Calculate home game winning percentage and number of away games each year\n",
    "home_games_group = df.groupby(['year', 'team_abbreviation_home'])\n",
    "home_games_stats = home_games_group.agg(\n",
    "    home_win_percentage_year=('home_result', 'mean'),\n",
    "    home_games_played=('home_result', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate away game winning percentage and number of away games each year\n",
    "away_games_group = df.groupby(['year', 'team_abbreviation_away'])\n",
    "away_games_stats = away_games_group.agg(\n",
    "    away_win_percentage_year=('away_result', 'mean'),\n",
    "    away_games_played=('away_result', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Create new columns in the original dataframe to store the calculated statistics\n",
    "df = df.merge(home_games_stats, how='left', left_on=['year', 'team_abbreviation_home'], right_on=['year', 'team_abbreviation_home'])\n",
    "df = df.merge(away_games_stats, how='left', left_on=['year', 'team_abbreviation_away'], right_on=['year', 'team_abbreviation_away'])\n",
    "\n",
    "#Create column for days since first date in new dataframe (01/02/2004)\n",
    "reference_date = datetime(2004, 1, 2)\n",
    "df['days_since_reference'] = (df['game_date'] - reference_date).dt.days\n",
    "\n",
    "#Filter to only seasons with 30+ home and away games (90/3 due to three officials for every game)\n",
    "df2 = df[(df['home_games_played'] > 90) & (df['away_games_played'] > 90)]\n",
    "\n",
    "#Create dataframe for propensity score matching\n",
    "psm_data = df2[['official_id', 'game_id', 'home_win_percentage_year', 'away_win_percentage_year', 'year', 'days_since_reference', 'home_result']].copy()\n",
    "psm_data.rename(columns={\n",
    "    'home_win_percentage_year': 'home_pct',\n",
    "    'away_win_percentage_year': 'away_pct',\n",
    "    'days_since_reference': 'time',\n",
    "    'home_result': 'result'\n",
    "}, inplace=True)\n",
    "psm_data = psm_data.astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "#Calculate propensity scores with logistic regression\n",
    "def calculate_propensity_scores(data):\n",
    "    X = data[['home_pct', 'away_pct', 'time']]\n",
    "    y = data['result']\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, y)\n",
    "    propensity_scores = model.predict_proba(X)[:, 1]\n",
    "    return propensity_scores\n",
    "\n",
    "# Initialize a new column with NaNs\n",
    "psm_data['propensity_score'] = None\n",
    "\n",
    "# Loop over each official_id group and assign the propensity scores directly\n",
    "for official_id, group_data in psm_data.groupby('official_id'):\n",
    "    psm_data.loc[group_data.index, 'propensity_score'] = calculate_propensity_scores(group_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:06:33.548110Z",
     "start_time": "2023-09-17T20:06:33.279606Z"
    }
   },
   "id": "7e9a2ea9f6125a42"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "#Find closest match by matching propensity scores by absolute difference, but ensure the match is another referee and not the same game (since there are multiple referees per game)\n",
    "def find_closest_match(row):\n",
    "    valid_matches = psm_data[(psm_data['official_id'] != row['official_id']) & (psm_data['game_id'] != row['game_id'])]\n",
    "    if valid_matches.empty:\n",
    "        return pd.Series([None]*len(row), index=['matched_' + col for col in psm_data.columns])\n",
    "    closest_match = valid_matches.loc[(valid_matches['propensity_score'] - row['propensity_score']).abs().idxmin()]\n",
    "    return closest_match.add_prefix('matched_')\n",
    "\n",
    "#Concatenate matches to the row it is being matched to\n",
    "result_df = psm_data.apply(lambda row: pd.concat([row, find_closest_match(row)]), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:09:08.139536Z",
     "start_time": "2023-09-17T20:06:33.550094Z"
    }
   },
   "id": "d5e9c6f16044feb4"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Separate original and matched data\n",
    "original_data = result_df.iloc[:, :len(psm_data.columns)]\n",
    "matched_data = result_df.iloc[:, len(psm_data.columns):]\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "effect_sizes = {}\n",
    "\n",
    "# Loop through each official_id\n",
    "for official_id in original_data['official_id'].unique():\n",
    "    # Get the rows for the current official_id from both original and matched data\n",
    "    original_rows = original_data[original_data['official_id'] == official_id]\n",
    "    matched_rows = matched_data[original_data['official_id'] == official_id]\n",
    "    \n",
    "    # Conduct paired t-test (on 'result' column as an example)\n",
    "    t_statistic, p_value = ttest_rel(original_rows['result'], matched_rows['matched_result'].dropna())\n",
    "\n",
    "    # Calculate Cohen's d as the effect size\n",
    "    cohen_d = (original_rows['result'].mean() - matched_rows['matched_result'].mean()) / np.sqrt(((original_rows['result'].std() ** 2 + matched_rows['matched_result'].std() ** 2) / 2))\n",
    "    \n",
    "    # Store the results in the dictionary\n",
    "    effect_sizes[official_id] = {\"t_statistic\": t_statistic, \"p_value\": p_value, \"cohen_d\": cohen_d}\n",
    "\n",
    "# Convert the results dictionary to a DataFrame\n",
    "effect_size_df = pd.DataFrame.from_dict(effect_sizes, orient='index')\n",
    "\n",
    "# Merge to get the first_name and last_name from the original df\n",
    "effect_size_df.reset_index(inplace=True)\n",
    "effect_size_df.rename(columns={'index': 'official_id'}, inplace=True)\n",
    "df['official_id'] = df['official_id'].astype('float64')\n",
    "effect_size_df = effect_size_df.merge(df[['official_id', 'first_name', 'last_name']], on='official_id', how='left')\n",
    "effect_size_df = effect_size_df.drop_duplicates()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:09:08.227767Z",
     "start_time": "2023-09-17T20:09:08.143782Z"
    }
   },
   "id": "807fe517eaddd6ea"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "       official_id  t_statistic   p_value   cohen_d first_name  last_name\n0           1189.0     0.551180  0.581830  0.036630       Phil   Robinson\n425         1200.0    -0.454481  0.649689 -0.026031       Greg    Willard\n986         1190.0    -0.760564  0.447278 -0.044801      Eddie       Rush\n1534        1159.0     0.000000  1.000000  0.000000      Kevin       Fehr\n2130        1195.0    -0.730051  0.465605 -0.035312    Derrick   Stafford\n...            ...          ...       ...       ...        ...        ...\n60423     204059.0     0.000000  1.000000  0.000000      Tyler       Ford\n60818     204058.0    -1.426926  0.154957 -0.129985  Gediminas  Petraitis\n61208    1626302.0    -0.420667  0.674506 -0.045357      Aaron      Smith\n61532    1627962.0    -0.491094  0.624098 -0.054819      Jacyn      Goble\n61820    1626300.0    -0.376821  0.706873 -0.042678        Ray     Acosta\n\n[86 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>official_id</th>\n      <th>t_statistic</th>\n      <th>p_value</th>\n      <th>cohen_d</th>\n      <th>first_name</th>\n      <th>last_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1189.0</td>\n      <td>0.551180</td>\n      <td>0.581830</td>\n      <td>0.036630</td>\n      <td>Phil</td>\n      <td>Robinson</td>\n    </tr>\n    <tr>\n      <th>425</th>\n      <td>1200.0</td>\n      <td>-0.454481</td>\n      <td>0.649689</td>\n      <td>-0.026031</td>\n      <td>Greg</td>\n      <td>Willard</td>\n    </tr>\n    <tr>\n      <th>986</th>\n      <td>1190.0</td>\n      <td>-0.760564</td>\n      <td>0.447278</td>\n      <td>-0.044801</td>\n      <td>Eddie</td>\n      <td>Rush</td>\n    </tr>\n    <tr>\n      <th>1534</th>\n      <td>1159.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>Kevin</td>\n      <td>Fehr</td>\n    </tr>\n    <tr>\n      <th>2130</th>\n      <td>1195.0</td>\n      <td>-0.730051</td>\n      <td>0.465605</td>\n      <td>-0.035312</td>\n      <td>Derrick</td>\n      <td>Stafford</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>60423</th>\n      <td>204059.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>Tyler</td>\n      <td>Ford</td>\n    </tr>\n    <tr>\n      <th>60818</th>\n      <td>204058.0</td>\n      <td>-1.426926</td>\n      <td>0.154957</td>\n      <td>-0.129985</td>\n      <td>Gediminas</td>\n      <td>Petraitis</td>\n    </tr>\n    <tr>\n      <th>61208</th>\n      <td>1626302.0</td>\n      <td>-0.420667</td>\n      <td>0.674506</td>\n      <td>-0.045357</td>\n      <td>Aaron</td>\n      <td>Smith</td>\n    </tr>\n    <tr>\n      <th>61532</th>\n      <td>1627962.0</td>\n      <td>-0.491094</td>\n      <td>0.624098</td>\n      <td>-0.054819</td>\n      <td>Jacyn</td>\n      <td>Goble</td>\n    </tr>\n    <tr>\n      <th>61820</th>\n      <td>1626300.0</td>\n      <td>-0.376821</td>\n      <td>0.706873</td>\n      <td>-0.042678</td>\n      <td>Ray</td>\n      <td>Acosta</td>\n    </tr>\n  </tbody>\n</table>\n<p>86 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_size_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:09:08.234560Z",
     "start_time": "2023-09-17T20:09:08.228463Z"
    }
   },
   "id": "58d60f3bbc598bfc"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Create a 'Referee' column by combining 'first_name' and 'last_name'\n",
    "effect_size_df['Referee'] = effect_size_df['first_name'] + \" \" + effect_size_df['last_name']\n",
    "\n",
    "# Select top 10 rows based on 'p_value'\n",
    "top_10_p_value = effect_size_df.nsmallest(10, 'p_value')\n",
    "\n",
    "# Select and rename the columns as per your requirements\n",
    "PSM_results = top_10_p_value[['Referee', 'p_value', 't_statistic', 'cohen_d']]\n",
    "PSM_results.columns = ['Referee', 'p-value', 't statistic', \"cohen's d\"]\n",
    "PSM_results.to_csv('Outputs/PSM_Results.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T20:09:08.237622Z",
     "start_time": "2023-09-17T20:09:08.235208Z"
    }
   },
   "id": "3709160aa98d8abc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
